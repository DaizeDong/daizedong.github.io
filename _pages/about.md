---
permalink: /
title: "Biography"
excerpt: "About me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

[//]: # (I received my B.E. degree in Computer Science from [UESTC]&#40;https://en.uestc.edu.cn/&#41; in 2023.)

I am an incoming Ph.D. student in Computer Science at [Rutgers University, New Brunswick](https://www.rutgers.edu/new-brunswick), advised by [Prof. Hongyi Wang](https://hwang595.github.io/).
My research focuses on improving the efficiency of neural networks across algorithmic and system levels. Specifically, I am interested in the following areas:

1. Conditional Computation: Mixture of Experts, Sparse Activation Models
2. Model Compression: Pruning, Quantization, Knowledge Distillation
3. System Optimization: Accelerated Training, Low-Latency Inference, Efficient Kernel Design

## News

- [2025/04] Will join [Rutgers University, New Brunswick](https://www.rutgers.edu/new-brunswick) as a Ph.D. student!
- [2024/11] Open-sourced project [LLaMA-MoE v2](https://github.com/OpenSparseLLMs/LLaMA-MoE-v2) is released!
- [2024/09] One paper ([LLaMA-MoE](https://arxiv.org/abs/2406.16554)) is accepted by [EMNLP 2024](https://2024.emnlp.org/).
- [2024/05] One paper ([GraphsGPT](https://arxiv.org/abs/2402.02464)) is accepted by [ICML 2024](https://icml.cc/Conferences/2024).
- [2024/04] One paper ([iDAT](https://arxiv.org/abs/2403.15750)) is accepted by [ICME 2024](https://icml.cc/Conferences/2024) as oral.
- [2023/12] Open-sourced project [LLaMA-MoE](https://github.com/pjlab-sys4nlp/llama-moe) is released!
- [2023/05] One paper ([PAD-Net](https://aclanthology.org/2023.acl-long.803.pdf)) is accepted by [ACL 2023](https://2023.aclweb.org/).
- [2022/10] One paper ([SparseAdapter](https://aclanthology.org/2022.findings-emnlp.160.pdf)) is accepted by [EMNLP 2022](https://2022.emnlp.org/).
- [2022/08] One paper ([SD-Conv](https://openaccess.thecvf.com/content/WACV2023/papers/He_SD-Conv_Towards_the_Parameter-Efficiency_of_Dynamic_Convolution_WACV_2023_paper.pdf)) is accepted by [WACV 2023](https://wacv2023.thecvf.com/).

## Publications

1. Zhangyang Gao\*, **Daize Dong**\*, Cheng Tan, Jun Xia, Bozhen Hu, Stan Z. Li, A Graph is Worth K Words: Euclideanizing Graph using Pure Transformer, ICML 2024. [[Paper](https://arxiv.org/abs/2402.02464)]
2. Jiacheng Ruan, Jingsheng Gao, Mingye Xie, **Daize Dong**, Suncheng Xiang, Ting Liu, Yuzhuo Fu, iDAT: inverse Distillation Adapter-Tuning, ICME 2024 (Oral). [[Paper](https://arxiv.org/abs/2403.15750)]
3. Shwai He, Liang Ding, **Daize Dong**, Boan Liu, Fuqiang Yu, Dacheng Tao, PAD-Net: An Efficient Framework for Dynamic Networks, ACL 2023. [[Paper](https://aclanthology.org/2023.acl-long.803.pdf)]
3. Shwai He, Liang Ding, **Daize Dong**, Miao Zhang, Dacheng Tao, SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters, EMNLP 2022. [[Paper](https://aclanthology.org/2022.findings-emnlp.160.pdf)]
4. Shwai He, Chenbo Jiang, **Daize Dong**, Liang Ding, SD-Conv: Towards the Parameter-Efficiency of Dynamic Convolution, WACV 2023. [[Paper](https://openaccess.thecvf.com/content/WACV2023/papers/He_SD-Conv_Towards_the_Parameter-Efficiency_of_Dynamic_Convolution_WACV_2023_paper.pdf)]

## Projects

1. Tong Zhu, Xiaoye Qu, **Daize Dong**, Jiacheng Ruan, Jingqi Tong, Conghui He, Yu Cheng, LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training, EMNLP 2024. [[Code](https://github.com/pjlab-sys4nlp/llama-moe)] [[Paper](https://arxiv.org/abs/2406.16554)]
2. Xiaoye Qu, **Daize Dong**, Xuyang Hu, Tong Zhu, Weigao Sun, Yu Cheng, LLaMA-MoE v2: Exploring Sparsity of LLaMA from Perspective of Mixture-of-Experts with Post-Training**. [[Code](https://github.com/OpenSparseLLMs/LLaMA-MoE-v2)] [[Paper](https://arxiv.org/abs/2411.15708)]

[//]: # (## Service)

[//]: # ()

[//]: # (### Reviewer)

[//]: # (- ACL: 2024, 2025)
