---
title: "01:198:461 Machine Learning Principles (TA)"
collection: teaching
type: "Recitation"
venue: "Rutgers University"
semester: "Fall 2025"
permalink: /teaching/2025-fall-cs461/
date: 2025-09-05
location: "Piscataway, NJ"
---

Machine Learning Principles (01:198:461, Sec. 06) surveys core ML foundations: linear/logistic regression, regularization, decision trees/forests, probabilistic models, kernels/SVMs, and an introduction to deep learning (CNNs, RNNs/LSTMs, Transformers).

---

Welcome! Slides and materials for the recitations will appear here.

### Recitation 01 ‚Äî Distributions & Hypothesis Testing (Sep 15, 2025)

- üìÑ **PDF (view online):** [RE01-Distributions_and_Significance_Tests.pdf]({{ '/files/teaching/2025-fall-cs461/RE01-Distributions_and_Significance_Tests.pdf' | relative_url }})
- ‚¨áÔ∏è **PPTX (download):** [RE01-Distributions_and_Significance_Tests.pptx]({{ '/files/teaching/2025-fall-cs461/RE01-Distributions_and_Significance_Tests.pptx' | relative_url }})

<details markdown="1">
<summary>What we covered</summary>

- Distributions
    - Continuous (Uniform, Gaussian, Student‚Äôs *t*, Laplace)
    - Discrete (Bernoulli, Binomial)
- Hypothesis Testing
    - P-values
    - œá¬≤ tests

</details>

### Recitation 02 ‚Äî Decision Trees & Data Generation (Sep 22, 2025)

- ‚¨áÔ∏è **Materials (download):** [RE02-Decision_Trees_and_Data_Generation.zip]({{ '/files/teaching/2025-fall-cs461/RE02-Decision_Trees_and_Data_Generation.zip' | relative_url }})
- üéûÔ∏è **Zoom Recording (view online):** [RE02-Decision_Trees_and_Data_Generation.mp4](https://drive.google.com/file/d/1d5RsILB9OBxPe7G3qG4hJLwTgsM01FRg/view?usp=drive_link)

<details markdown="1">
<summary>What we covered</summary>

- Recursive tree building
    - Use a `Node` class to represent each node in the tree
    - Store necessary information such as column names, threshold, left/right children, parent, and class labels
    - Use a recursive function to split the data based on the best feature at each node
- Synthetic data generation
    - Traversing the tree from a random leaf node up to the root
    - Estimating feature values along the way
    - Handling edge cases, such as when a leaf node has too few samples or there are missing values

</details>

---

### Office Hours

- **Time:** Not yet scheduled

- **Location:** Not yet scheduled

- **Contact:** daize.dong@rutgers.edu

